{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting hourly temperature data from NOAA ISD (integrated surface database) weather data\n",
    "\n",
    "ish_parser python module is from:\n",
    "https://github.com/haydenth/ish_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# boilerplate includes\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "#mpl.use('nbagg')\n",
    "import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#import mpld3 # for outputting interactive html figures\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import ish_parser\n",
    "import gzip\n",
    "import ftplib\n",
    "import io\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib notebook\n",
    "plt.style.use('seaborn-notebook')\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PARAMETERS (might be overridden by a calling script)\n",
    "\n",
    "# if not calling from another script (batch), SUBNOTEBOOK_FLAG might not be defined\n",
    "try:\n",
    "    SUBNOTEBOOK_FLAG\n",
    "except NameError:\n",
    "    SUBNOTEBOOK_FLAG = False\n",
    "    \n",
    "# Not calling as a sub-script? define params here\n",
    "if not SUBNOTEBOOK_FLAG:\n",
    "    \n",
    "    # SET PARAMETER VARIABLES HERE UNLESS CALLING USING %run FROM ANOTHER NOTEBOOK\n",
    "    \n",
    "    STATION_CALLSIGN = 'PHTO'\n",
    "\n",
    "    USE_CACHED_STATION_H5_FILES = True\n",
    "    SUPPRESS_FIGURE_DISPLAY = False\n",
    "\n",
    "    DATADIR = '../data/temperatures/ISD'\n",
    "    OUTDIR = '../data/temperatures'\n",
    "\n",
    "    FTPHOST = 'ftp.ncdc.noaa.gov'\n",
    "    FETCH_STATIONS_LIST_FILE = True\n",
    "    \n",
    "print(\"Fetching and parsing \",STATION_CALLSIGN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Locate the station info...\n",
    "\n",
    "Could either do it by hand, or else try to get all the data associated with a single station callsign. The latter seems like a cooler way to go... but have to be careful that the stations really are the same and the data is comparable for our purposes.\n",
    "\n",
    "stations list: ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if FETCH_STATIONS_LIST_FILE:\n",
    "    # fetch a fresh copy of the stations list\n",
    "    with open(os.path.join(DATADIR,'isd-history.txt'),'wb') as fh:\n",
    "        with ftplib.FTP(host=FTPHOST) as ftpconn:\n",
    "            ftpconn.login()\n",
    "            ftpconn.retrbinary('RETR '+'/pub/data/noaa/isd-history.txt', fh.write)\n",
    "            ftpconn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to parse stations list file\n",
    "def read_isd_history_stations_list(filename, skiprows=22):\n",
    "    \"\"\"Read and parse stations information from isd_history.txt file\"\"\"\n",
    "    fwfdef = (( ('USAF', (6, str)),\n",
    "                ('WBAN', (5, str)),\n",
    "                ('STATION NAME', (28, str)),\n",
    "                ('CTRY', (4, str)),\n",
    "                ('ST', (2, str)),\n",
    "                ('CALL', (5, str)),\n",
    "                ('LAT', (7, str)),\n",
    "                ('LON', (8, str)),\n",
    "                ('EVEV', (7, str)),\n",
    "                ('BEGIN', (8, str)),\n",
    "                ('END', (8, str)),\n",
    "                ))\n",
    "    names = []\n",
    "    colspecs = []\n",
    "    converters = {}\n",
    "    i = 0\n",
    "    for k,v in fwfdef:\n",
    "        names.append(k)\n",
    "        colspecs.append((i, i+v[0]+1))\n",
    "        i += v[0]+1\n",
    "        converters[k] = v[1]\n",
    "    stdf = pd.read_fwf(filename, skiprows=skiprows,\n",
    "                       names=names,\n",
    "                       colspecs=colspecs,\n",
    "                       converters=converters)\n",
    "    return stdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# actually parse the file\n",
    "stationsdf = read_isd_history_stations_list(\n",
    "    os.path.join(DATADIR,'isd-history.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pick just the info associated with the station we want\n",
    "station_info = stationsdf[stationsdf['CALL'] == STATION_CALLSIGN]\n",
    "station_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # maybe only use a subset of these entires\n",
    "# station_info = station_info.iloc[2:3]\n",
    "# station_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the actual data\n",
    "data files are at:ftp://ftp.ncdc.noaa.gov/pub/data/noaa/{YEAR}/{USAF}-{WBAN}-{YEAR}.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_ish_data(usaf_id, wban_id, years_to_get, \n",
    "                      ftp_host=FTPHOST,\n",
    "                      verbose=True):\n",
    "    parser = ish_parser.ish_parser()\n",
    "    with ftplib.FTP(host=ftp_host) as ftpconn:\n",
    "        ftpconn.login()\n",
    "        for year in years_to_get:\n",
    "            ftp_file = \"/pub/data/noaa/{YEAR}/{USAF}-{WBAN}-{YEAR}.gz\".format(\n",
    "                USAF=usaf_id, WBAN=wban_id, YEAR=year)\n",
    "            if verbose:\n",
    "                print(ftp_file)\n",
    "            # read the whole file and save it to a BytesIO (stream)\n",
    "            response = io.BytesIO()\n",
    "            try:\n",
    "                ftpconn.retrbinary('RETR '+ftp_file, response.write)\n",
    "            except ftplib.error_perm as err:\n",
    "                if str(err).startswith('550 '):\n",
    "                    print('ERROR:', err)\n",
    "                else:\n",
    "                    raise\n",
    "            # decompress and parse each line \n",
    "            response.seek(0) # jump back to the beginning of the stream\n",
    "            with gzip.open(response, mode='rb') as gzstream:\n",
    "                for line in gzstream:\n",
    "                    parser.loads(line.decode('latin-1'))\n",
    "    # get the list of all reports\n",
    "    reports = parser.get_reports()\n",
    "    if verbose:\n",
    "        print(len(reports), \"records\")\n",
    "    # just return None if no records were found\n",
    "    if len(reports) <= 0:\n",
    "        return None\n",
    "    # convert to a pandas dataframe\n",
    "    foo = pd.DataFrame.from_records(\n",
    "                ((r.datetime, r.air_temperature.get_numeric()) for r in reports),\n",
    "                columns=['datetime','AT'],\n",
    "                index='datetime')\n",
    "    foo.index = pd.to_datetime(foo.index) # convert the index to pandas datetime objects\n",
    "    foo.dropna(inplace=True) # drop entires which don't have an AT value\n",
    "    foo.sort_index(inplace=True) # go ahead and ensure it is sorted\n",
    "    return foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = None\n",
    "for _,row in station_info.iterrows():\n",
    "    usaf_id = row['USAF']\n",
    "    wban_id = row['WBAN']\n",
    "    years_to_get = range(int(row['BEGIN'][0:4]), int(row['END'][0:4])+1)\n",
    "    print('####', usaf_id, wban_id, years_to_get)\n",
    "\n",
    "    station_h5file = os.path.join(DATADIR,\n",
    "                \"{USAF}-{WBAN}-AT.h5\".format(USAF=usaf_id, WBAN=wban_id))\n",
    "\n",
    "    station_df = None\n",
    "    if USE_CACHED_STATION_H5_FILES:\n",
    "        if os.path.isfile(station_h5file):\n",
    "            print(\"Using cached file: '{}'\".format(station_h5file))\n",
    "            station_df = pd.read_hdf(station_h5file, 'table')\n",
    "\n",
    "    if station_df is None:\n",
    "        station_df = download_ish_data(usaf_id, wban_id, years_to_get, ftp_host=FTPHOST)\n",
    "    \n",
    "    if station_df is None:\n",
    "        print(\"WARNING: No data found for {} {} {}\".format(usaf_id, wban_id, years_to_get))\n",
    "    else:\n",
    "    \n",
    "        # Save this station's individual data\n",
    "        print(\"Saving station data to: '{}'\".format(station_h5file))\n",
    "        station_df.to_hdf(station_h5file,'table')\n",
    "\n",
    "        # Combine into single dataset\n",
    "        if df is None:\n",
    "            df = station_df.copy(deep=True)\n",
    "        else:\n",
    "            # @TCC TODO: Maybe use some more clever logic than just \"combine_first\"\n",
    "            df = df.combine_first(station_df)\n",
    "\n",
    "# ensure the final combined dataset is sorted\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# save the combined datafram\n",
    "combined_AT_filename = \"{}_AT.h5\".format(STATION_CALLSIGN)\n",
    "print(\"Saving combined data to: '{}'\".format(combined_AT_filename))\n",
    "df.to_hdf(os.path.join(DATADIR, combined_AT_filename),'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot (decomment to enable)\n",
    "\n",
    "# if SUPPRESS_FIGURE_DISPLAY:\n",
    "#     plt.ioff()\n",
    "# ax = df.plot(title=STATION_CALLSIGN, marker='.')\n",
    "# ax.set_ylabel('air temperature [$\\degree$ C]')\n",
    "# plt.savefig(os.path.join(OUTDIR,'{}_AT_orig.png'.format(STATION_CALLSIGN)))\n",
    "# plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Distribution plot (decomment to enable)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(1,1,1)\n",
    "# sns.kdeplot(df['AT'], bw=.5, ax=ax, legend=False)\n",
    "# ax.set_xlabel('air temperature [$\\degree$C]')\n",
    "# ax.set_ylabel('proportion of readings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
